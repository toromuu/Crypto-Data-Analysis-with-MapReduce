# SSH - Manual work

Since Hadoop's Cluster has not AWS CLI installed and we do not have sudo privileges we need to send the generated Graphs and Tables to S3 bucket, we need to follow below steps:

1. First we will run `configure.sh` script in Hadoop Cluster.

2. We will manually copy the ssh-rsa public key generated by `configure.sh` script and we will copy it into AWS LAB instance, which has installed aws cli and has enough credentials for S3 bucket upload.

```
alucloud$ID@hadoopmaster:~$ cat ssh-key.pub
```

3. Copy the text and add it into .ssh/authorized_keys in the AWS LAB instance:

```
alucloud$ID@lab:~$ vim .ssh/authorized_keys 
```

4. We can now access from Hadoop Cluster to AWS LAB instance without adding a password:

```
alucloud$ID@hadoopmaster:~$ ssh -o StrictHostKeyChecking=no -i ssh-key alucloud$ID@lab2.cursocloudaws.net
```

5. This will allow us to scp the generated Graphs and Tables to the AWS LAB instance running the following command in the `launch.sh` script:

```
alucloud$ID@hadoopmaster:~$ scp -o StrictHostKeyChecking=no -i ssh-key example.txt alucloud$ID@lab2.cursocloudaws.net:/home/alucloud$ID/.
```

6. To upload the files into S3 we will use `s3_upload.sh` script.